{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a9d9df-ad3b-4133-9b6d-5079b3509b1d",
   "metadata": {},
   "source": [
    "## Loading the Multi-Modal Framing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1830b30-761f-4a42-a706-4debf0e49e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bc2552-8ae9-406a-933f-74b5092a10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset(\"copenlu/mm-framing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b34cfc0-b3e0-4aa2-bb8b-be68a522303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['full', 'valid_framing_subset'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598504-edc6-4dde-8ff0-bb55023fe24f",
   "metadata": {},
   "source": [
    "## According to the paper, the valid_framing_subset was created by applying additional filtering on the full dataset in order to focus on the articles that are suitable for framing analysis. \n",
    "### Specifically:\n",
    "- They removed articles for which the framing model predicted “None”\n",
    "- They removed articles whose text length was below 100 words\n",
    "- They removed articles whose topic was “sports” or “media”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8054c7-2e76-4f74-a798-5201d9aeb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "framing_subset = raw_ds['valid_framing_subset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcaa0b-5e40-4d83-8830-4d2d58d996f2",
   "metadata": {},
   "source": [
    "### Column name descriptions:\n",
    "- uuid - Unique ID for each article\n",
    "- title - Title of the article\n",
    "- date_publish - Publication date\n",
    "- source_domain - Domain of the publisher\n",
    "- url - Article URL\n",
    "- political_leaning - Political leaning of the publisher\n",
    "### Annotations\n",
    "- text-topic - Article topic generated from article text\n",
    "- text-topic-exp - Article topic explanation\n",
    "- text-entity-name - Main entity in article text\n",
    "- text-entity-sentiment - Sentiment towards main entity\n",
    "- text-entity-sentiment-exp - Explanation of text sentiment\n",
    "- text-generic-frame - Generic Frame used in Article text\n",
    "- text-generic-frame-exp - Generic Frame in text explanation\n",
    "- text-issue-frame - Issue Frame used in article text\n",
    "- text-issue-frame-exp - Issue Frame explanation\n",
    "- img-generic-frame - Generic Frame used in Article Image\n",
    "- img-frame-exp - Generic Frame in image explanation\n",
    "- img-entity-name - Main subject in Article Image\n",
    "- img-entity-sentiment - Sentiment towards the subject in Article image\n",
    "- img-entity-sentiment-exp - Explanation of image sentiment\n",
    "- gpt-topic - Consolidated topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94774f09-1307-44d9-8580-ad6207e0ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uuid': '000002bf-ddb3-4386-9149-55328ce1c651',\n",
       " 'title': \"Philippine military condemns Chinese coast guard's use of water cannon on its boat in disputed sea\",\n",
       " 'date_publish': '2023-08-06 00:10:52',\n",
       " 'source_domain': 'www.washingtontimes.com',\n",
       " 'url': 'https://www.washingtontimes.com/news/2023/aug/6/philippine-military-condemns-chinese-coast-guards-/?utm_source=RSS_Feed&utm_medium=RSS',\n",
       " 'political_leaning': 'right_lean',\n",
       " 'text-topic': 'South China Sea Dispute',\n",
       " 'text-topic-exp': \"The article discusses a confrontation between the Philippine military and a Chinese coast guard ship over a Philippine-occupied shoal in the South China Sea. The article mentions the involvement of several countries, including the United States, Australia, and Japan, expressing concern over the actions of the Chinese ship. The article also mentions the long-standing territorial conflicts in the South China Sea and the international rulings that invalidated China's territorial claims.\",\n",
       " 'text-entity-name': 'Philippine military',\n",
       " 'text-entity-sentiment': 'Negative',\n",
       " 'text-entity-sentiment-exp': \"The article describes the Philippine military as condemning the 'excessive and offensive' actions of a Chinese coast guard ship, and expresses concern from several countries over the actions of the Chinese ship. The Philippine military also calls on the Chinese coast guard to act with prudence and be responsible to prevent miscalculations and accidents.\",\n",
       " 'text-generic-frame': \"['security', 'legality', 'regulation', 'policy']\",\n",
       " 'text-generic-frame-exp': \"The article discusses a tense confrontation between the Philippine military and Chinese coast guard ships in the South China Sea, which is a strategic waterway and a delicate fault line in the rivalry between the United States and China in the region. The Chinese coast guard ship's actions are described as violating international law, including the 1982 U.N. Convention on the Law of the Sea. Several countries expressed concern over the actions of the Chinese ship, with the United States renewing a warning that it's obliged to defend its treaty ally when Filipino public vessels and forces come under an armed attack. The article also mentions China's rejection of international rulings that invalidated Beijing’s vast territorial claims, such as that of 2016 by the Permanent Court of Arbitration, an international body based in The Hague. China's actions and responses are also discussed in relation to the U.S. and its military presence in the region.\",\n",
       " 'text-issue-frame': 'Geopolitical Tension',\n",
       " 'text-issue-frame-exp': \"The article discusses a tense confrontation between the Philippine military and a Chinese coast guard ship over a disputed shoal in the South China Sea. The incident is part of long-seething territorial conflicts involving several countries, including China, the Philippines, Vietnam, Malaysia, Taiwan, and Brunei. The article mentions the involvement of the United States, Australia, and Japan, expressing concern over the actions of the Chinese coast guard ship. This suggests that the incident is not just a local issue but a geopolitical one with implications for regional peace and stability. The use of terms like 'Asian flashpoint' and 'delicate fault line' in the rivalry between the United States and China in the region further supports this framing.\",\n",
       " 'img-generic-frame': \"['security']\",\n",
       " 'img-frame-exp': 'The image shows military equipment and personnel, indicating a focus on security and defense. However, there is no clear indication of any other specific frame.',\n",
       " 'img-entity-name': 'Navy Ship',\n",
       " 'img-entity-sentiment': 'neutral',\n",
       " 'img-entity-sentiment-exp': \"The image shows a navy ship with no visible personnel, and the portrayal is factual and neutral, focusing on the ship's equipment and structure.\",\n",
       " 'gpt-topic': 'War'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framing_subset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0038146a-7903-4f6a-8eaf-2d4cf2283a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "TEXT_COL = \"text-generic-frame-exp\"      \n",
    "LABEL_COL = \"text-generic-frame\"         \n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946e8816-0a40-40d0-a1b4-d50d0282665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6596a4b58b7f4730830ac2bf4f20aba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/153991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0795557d6fe14e0585dadea7843295b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/153991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0054b6065943daabea66f40abe85d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f13ca43acc849e881d7c27c6a9965ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2462585c260246189cea8639eb76ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mickey\\anaconda3\\envs\\milestone3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e3283424504d198734863081cc8484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c322f2004d4b0ba2ef5f73f831cf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbd803a4645412b8d37a3bb3457de13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 122566\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 15321\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 15321\n",
      "    })\n",
      "})\n",
      "num_labels: 14\n",
      "frames[:10] = ['cap&res', 'crime', 'culture', 'economic', 'fairness', 'health', 'legality', 'morality', 'policy', 'political']\n"
     ]
    }
   ],
   "source": [
    "# Convert string to a list and ensure no NaN or trailing whitespace\n",
    "def _clean(ds):\n",
    "    labels = ds[LABEL_COL]\n",
    "    if isinstance(labels, str):\n",
    "        labels = ast.literal_eval(labels)\n",
    "    ds[LABEL_COL] = labels if labels else []\n",
    "    txt = ds.get(TEXT_COL) or \"\"\n",
    "    ds[\"__text__\"] = txt.strip()\n",
    "    return ds\n",
    "\n",
    "# filter to fraiming subset of data, clean, and drop anything with no text\n",
    "\n",
    "framing_subset = raw_ds[\"valid_framing_subset\"]\n",
    "framing_subset = framing_subset.map(_clean)\n",
    "framing_subset = framing_subset.filter(lambda e: len(e[\"__text__\"]) > 0)\n",
    "df = framing_subset.to_pandas()\n",
    "df = df.drop_duplicates(subset=\"__text__\")\n",
    "framing_subset = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "split_ds = framing_subset.train_test_split(test_size=0.2, seed=42)\n",
    "temp_split = split_ds[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "ds = DatasetDict({\n",
    "    \"train\": split_ds[\"train\"],\n",
    "    \"validation\": temp_split[\"train\"],\n",
    "    \"test\": temp_split[\"test\"]\n",
    "})\n",
    "\n",
    "\n",
    "# Build the multi-label space of all possible frames\n",
    "frames = set()\n",
    "for row in ds[\"train\"]:\n",
    "    frames.update(row[LABEL_COL])\n",
    "frames = sorted(frames)\n",
    "mlb = MultiLabelBinarizer(classes=frames)\n",
    "mlb.fit([[]])  \n",
    "num_labels = len(frames)\n",
    "\n",
    "\n",
    "# Convert labels to multi-hot vectors\n",
    "def _binarize(ds):\n",
    "    y = mlb.transform([ds[LABEL_COL]])[0].astype(np.float32)\n",
    "    ds[\"labels\"] = y\n",
    "    return ds\n",
    "ds = ds.map(_binarize)\n",
    "\n",
    "# Tokenize\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def _tok(batch):\n",
    "    return tok(batch[\"__text__\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "ds = ds.map(_tok, batched=True, remove_columns=[c for c in ds[\"train\"].column_names if c not in [\"labels\"]])\n",
    "\n",
    "print(ds)\n",
    "print(\"num_labels:\", num_labels)\n",
    "print(\"frames[:10] =\", frames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25dd54cc-9064-4828-90d4-05e1636f0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Mickey\\AppData\\Local\\Temp\\ipykernel_67864\\2754567959.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7661' max='7661' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7661/7661 29:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Avg Precision Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>0.984846</td>\n",
       "      <td>0.986489</td>\n",
       "      <td>0.996393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=7661, training_loss=0.060091103372771786, metrics={'train_runtime': 1763.9242, 'train_samples_per_second': 69.485, 'train_steps_per_second': 4.343, 'total_flos': 3.225252250215629e+16, 'train_loss': 0.060091103372771786, 'epoch': 1.0})\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "# Eval metrics\n",
    "best_thresh = 0.5  \n",
    "epochs = 1\n",
    "\n",
    "def _compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))          \n",
    "    preds = (probs >= best_thresh).astype(int)\n",
    "    out = {}\n",
    "    out[\"f1_macro\"] = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    out[\"f1_micro\"] = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    try:\n",
    "        out[\"avg_precision_macro\"] = average_precision_score(labels, probs, average=\"macro\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "# Build model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Define training arguments and trianing object\n",
    "# fp16 and per_device are controling GPU may need to tweak for diff computer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mmf_deberta_v3\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    report_to=[], \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=_compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4783166-61fb-4b4a-9129-98c8b4f190ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.60 (macro-F1=0.985)\n"
     ]
    }
   ],
   "source": [
    "# Tune decision threshold on the validation set\n",
    "val_out = trainer.predict(ds[\"validation\"])\n",
    "val_logits, val_labels = val_out.predictions, val_out.label_ids\n",
    "val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "def tune_threshold(probs, labels):\n",
    "    candidates = np.linspace(0.1, 0.9, 17)\n",
    "    best, best_f1 = 0.5, -1\n",
    "    for t in candidates:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best, best_f1 = t, f1\n",
    "    return best, best_f1\n",
    "\n",
    "best_thresh, best_f1 = tune_threshold(val_probs, val_labels)\n",
    "print(f\"Best threshold : {best_thresh:.2f} (macro-F1={best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0765e1b-abfb-4dda-820e-6550b48a710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to mmf_deberta_v3/best\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"mmf_deberta_v3/best\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model/tokenizer\n",
    "trainer.save_model(save_dir)\n",
    "tok.save_pretrained(save_dir)\n",
    "\n",
    "# Save frames and threshold\n",
    "with open(os.path.join(save_dir, \"frames.json\"), \"w\") as f:\n",
    "    json.dump(frames, f, indent=2)\n",
    "with open(os.path.join(save_dir, \"threshold.json\"), \"w\") as f:\n",
    "    json.dump({\"global\": float(best_thresh)}, f, indent=2)\n",
    "\n",
    "print(\"Saved to\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8aa9280-a317-47bb-9ceb-b928c83e7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN-VAL] Exact duplicates: 0\n",
      "[TRAIN-TEST] Exact duplicates: 0\n",
      "[VAL-TEST] Exact duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "raw_ds = load_dataset(\"copenlu/mm-framing\")\n",
    "\n",
    "framing_subset = raw_ds[\"valid_framing_subset\"]\n",
    "framing_subset = framing_subset.map(_clean)\n",
    "framing_subset = framing_subset.filter(lambda e: len(e[\"__text__\"]) > 0)\n",
    "df = framing_subset.to_pandas()\n",
    "df = df.drop_duplicates(subset=\"__text__\")\n",
    "\n",
    "framing_subset = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "split_ds = framing_subset.train_test_split(test_size=0.2, seed=42)\n",
    "temp_split = split_ds[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "ds_raw = DatasetDict({\n",
    "    \"train\": split_ds[\"train\"],\n",
    "    \"validation\": temp_split[\"train\"],\n",
    "    \"test\": temp_split[\"test\"]\n",
    "})\n",
    "\n",
    "\n",
    "def exact_overlap(name_a, texts_a, name_b, texts_b):\n",
    "    set_a = set(texts_a)\n",
    "    set_b = set(texts_b)\n",
    "    exact_overlap = set_a.intersection(set_b)\n",
    "    print(f\"[{name_a}-{name_b}] Exact duplicates: {len(exact_overlap)}\")\n",
    "\n",
    "train_texts = [ex[\"__text__\"] for ex in ds_raw[\"train\"]]\n",
    "val_texts   = [ex[\"__text__\"] for ex in ds_raw[\"validation\"]]\n",
    "test_texts  = [ex[\"__text__\"] for ex in ds_raw[\"test\"]]\n",
    "\n",
    "\n",
    "exact_overlap(\"TRAIN\", train_texts, \"VAL\",  val_texts)\n",
    "exact_overlap(\"TRAIN\", train_texts, \"TEST\", test_texts)\n",
    "exact_overlap(\"VAL\",   val_texts,  \"TEST\",  test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e5a68-8f49-4881-ab99-9656004e5ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
