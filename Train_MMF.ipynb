{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a9d9df-ad3b-4133-9b6d-5079b3509b1d",
   "metadata": {},
   "source": [
    "## Loading the Multi-Modal Framing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1830b30-761f-4a42-a706-4debf0e49e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bc2552-8ae9-406a-933f-74b5092a10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset(\"copenlu/mm-framing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b34cfc0-b3e0-4aa2-bb8b-be68a522303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['full', 'valid_framing_subset'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598504-edc6-4dde-8ff0-bb55023fe24f",
   "metadata": {},
   "source": [
    "## According to the paper, the valid_framing_subset was created by applying additional filtering on the full dataset in order to focus on the articles that are suitable for framing analysis. \n",
    "### Specifically:\n",
    "- They removed articles for which the framing model predicted “None”\n",
    "- They removed articles whose text length was below 100 words\n",
    "- They removed articles whose topic was “sports” or “media”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8054c7-2e76-4f74-a798-5201d9aeb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "framing_subset = raw_ds['valid_framing_subset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcaa0b-5e40-4d83-8830-4d2d58d996f2",
   "metadata": {},
   "source": [
    "### Column name descriptions:\n",
    "- uuid - Unique ID for each article\n",
    "- title - Title of the article\n",
    "- date_publish - Publication date\n",
    "- source_domain - Domain of the publisher\n",
    "- url - Article URL\n",
    "- political_leaning - Political leaning of the publisher\n",
    "### Annotations\n",
    "- text-topic - Article topic generated from article text\n",
    "- text-topic-exp - Article topic explanation\n",
    "- text-entity-name - Main entity in article text\n",
    "- text-entity-sentiment - Sentiment towards main entity\n",
    "- text-entity-sentiment-exp - Explanation of text sentiment\n",
    "- text-generic-frame - Generic Frame used in Article text\n",
    "- text-generic-frame-exp - Generic Frame in text explanation\n",
    "- text-issue-frame - Issue Frame used in article text\n",
    "- text-issue-frame-exp - Issue Frame explanation\n",
    "- img-generic-frame - Generic Frame used in Article Image\n",
    "- img-frame-exp - Generic Frame in image explanation\n",
    "- img-entity-name - Main subject in Article Image\n",
    "- img-entity-sentiment - Sentiment towards the subject in Article image\n",
    "- img-entity-sentiment-exp - Explanation of image sentiment\n",
    "- gpt-topic - Consolidated topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0038146a-7903-4f6a-8eaf-2d4cf2283a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "TEXT_COL = \"text-generic-frame-exp\"      \n",
    "LABEL_COL = \"text-generic-frame\"         \n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946e8816-0a40-40d0-a1b4-d50d0282665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6f0a26f2544f49811a902e52468bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/153991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24559b4eb66445dc83b7f306dbf52640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/153991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ef349f66e848ddb73a291479ef6407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc664d51de19413193beda23561c42d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49978f1aa95d47d5a8dbd251c6ddd069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mickey\\anaconda3\\envs\\milestone3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6eed30728c46c1890ed7bfd873f589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5661da4e066460181da6c5385b9b56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a2d9645e4744b2a4a3b352b7b93c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 122566\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 15321\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 15321\n",
      "    })\n",
      "})\n",
      "num_labels: 14\n",
      "frames = ['cap&res', 'crime', 'culture', 'economic', 'fairness', 'health', 'legality', 'morality', 'policy', 'political', 'public_op', 'quality_life', 'regulation', 'security']\n"
     ]
    }
   ],
   "source": [
    "# Convert string to a list and ensure no NaN or trailing whitespace\n",
    "def Clean(ds):\n",
    "    \"\"\" Converts a string to a list\"\"\"\n",
    "    labels = ds[LABEL_COL]\n",
    "    if isinstance(labels, str):\n",
    "        labels = ast.literal_eval(labels)\n",
    "    ds[LABEL_COL] = labels \n",
    "    txt = ds.get(TEXT_COL) or \"\"\n",
    "    ds[\"__text__\"] = txt.strip()\n",
    "    return ds\n",
    "\n",
    "# filter to fraiming subset of data, clean, and drop anything with no text\n",
    "framing_subset = raw_ds[\"valid_framing_subset\"]\n",
    "framing_subset = framing_subset.map(Clean)\n",
    "framing_subset = framing_subset.filter(lambda e: len(e[\"__text__\"]) > 0)\n",
    "df = framing_subset.to_pandas()\n",
    "df = df.drop_duplicates(subset=\"__text__\")\n",
    "framing_subset = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "split_ds = framing_subset.train_test_split(test_size=0.2, seed=42)\n",
    "temp_split = split_ds[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "ds = DatasetDict({\n",
    "    \"train\": split_ds[\"train\"],\n",
    "    \"validation\": temp_split[\"train\"],\n",
    "    \"test\": temp_split[\"test\"]\n",
    "})\n",
    "\n",
    "\n",
    "# Build the multi-label space of all possible frames\n",
    "frames = set()\n",
    "for row in ds[\"train\"]:\n",
    "    frames.update(row[LABEL_COL])\n",
    "frames = sorted(frames)\n",
    "mlb = MultiLabelBinarizer(classes=frames)\n",
    "mlb.fit([[]])  \n",
    "num_labels = len(frames)\n",
    "\n",
    "\n",
    "def Binarize(ds):\n",
    "    \"\"\" Convert labels to multi-hot vectors \"\"\"\n",
    "    ds[\"labels\"] = mlb.transform([ds[LABEL_COL]])[0].astype(np.float32)\n",
    "    return ds\n",
    "ds = ds.map(Binarize)\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def Tok(batch):\n",
    "    \"\"\"Tokenize's the text\"\"\"\n",
    "    return tok(batch[\"__text__\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "ds = ds.map(Tok, batched=True, remove_columns=[c for c in ds[\"train\"].column_names if c not in [\"labels\"]])\n",
    "\n",
    "print(ds)\n",
    "print(\"num_labels:\", num_labels)\n",
    "print(\"frames =\", frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336d753b-e881-4250-8e98-fb99b0f15f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train > val: 0\n",
      "train > test: 0\n",
      "val > test: 0\n"
     ]
    }
   ],
   "source": [
    "# Data Leakage Test\n",
    "train_texts = set(split_ds[\"train\"][\"__text__\"])\n",
    "val_texts   = set(temp_split[\"train\"][\"__text__\"])\n",
    "test_texts  = set(temp_split[\"test\"][\"__text__\"])\n",
    "\n",
    "print(\"train > val:\", len(train_texts & val_texts))\n",
    "print(\"train > test:\", len(train_texts & test_texts))\n",
    "print(\"val > test:\", len(val_texts & test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25dd54cc-9064-4828-90d4-05e1636f0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Mickey\\AppData\\Local\\Temp\\ipykernel_32808\\1786979811.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7661' max='7661' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7661/7661 29:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>0.984769</td>\n",
       "      <td>0.986478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=7661, training_loss=0.0604492592105136, metrics={'train_runtime': 1761.6786, 'train_samples_per_second': 69.573, 'train_steps_per_second': 4.349, 'total_flos': 3.225252250215629e+16, 'train_loss': 0.0604492592105136, 'epoch': 1.0})\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "best_thresh = 0.5  \n",
    "epochs = 1\n",
    "\n",
    "def Compute_metrics(eval_pred):\n",
    "    \"\"\"Returns eval metrics for transformer\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))          \n",
    "    preds = (probs >= best_thresh).astype(int)\n",
    "    out = {}\n",
    "    out[\"f1_macro\"] = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    out[\"f1_micro\"] = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    return out\n",
    "\n",
    "# Build model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Define training arguments and trianing object\n",
    "# fp16 and per_device are controling GPU may need to tweak for diff computer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mmf_deberta_v3\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    report_to=[], \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=Compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4783166-61fb-4b4a-9129-98c8b4f190ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.10 → F1=0.9764\n",
      "Threshold=0.15 → F1=0.9796\n",
      "Threshold=0.20 → F1=0.9815\n",
      "Threshold=0.25 → F1=0.9825\n",
      "Threshold=0.30 → F1=0.9831\n",
      "Threshold=0.35 → F1=0.9837\n",
      "Threshold=0.40 → F1=0.9843\n",
      "Threshold=0.45 → F1=0.9845\n",
      "Threshold=0.50 → F1=0.9848\n",
      "Threshold=0.55 → F1=0.9848\n",
      "Threshold=0.60 → F1=0.9848\n",
      "Threshold=0.65 → F1=0.9848\n",
      "Threshold=0.70 → F1=0.9847\n",
      "Threshold=0.75 → F1=0.9844\n",
      "Threshold=0.80 → F1=0.9840\n",
      "Threshold=0.85 → F1=0.9834\n",
      "Threshold=0.90 → F1=0.9820\n",
      "Best threshold : 0.65 (F1=0.985)\n"
     ]
    }
   ],
   "source": [
    "# Tune threshold on validation set\n",
    "val_out = trainer.predict(ds[\"validation\"])\n",
    "val_logits, val_labels = val_out.predictions, val_out.label_ids\n",
    "val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "def tune_threshold(probs, labels):\n",
    "    \"\"\" Tune decision threshold and return best threshold and F1 score\"\"\"\n",
    "    log = []   \n",
    "    candidates = np.linspace(0.1, 0.9, 17)\n",
    "    best, best_f1 = 0.5, -1\n",
    "    for t in candidates:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "        print(f\"Threshold={t:.2f} → F1={f1:.4f}\")\n",
    "        log.append({\"threshold\": t, \"f1\": f1})\n",
    "        if f1 > best_f1:\n",
    "            best, best_f1 = t, f1\n",
    "    return best, best_f1, log\n",
    "\n",
    "best_thresh, best_f1, log = tune_threshold(val_probs, val_labels)\n",
    "print(f\"Best threshold : {best_thresh:.2f} (F1={best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09574bc3-f4cf-416a-8e3d-6fc6d4fceb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Performance:\n",
      "  F1 Macro: 0.9852\n",
      "  F1 Micro: 0.9864\n"
     ]
    }
   ],
   "source": [
    "# Final Eval on Test Set\n",
    "test_out = trainer.predict(ds[\"test\"])\n",
    "test_logits, test_labels = test_out.predictions, test_out.label_ids\n",
    "test_probs = 1 / (1 + np.exp(-test_logits))\n",
    "test_preds = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "\n",
    "test_f1_macro = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
    "test_f1_micro = f1_score(test_labels, test_preds, average=\"micro\", zero_division=0)\n",
    "\n",
    "print(\"Final Test Performance:\")\n",
    "print(f\"  F1 Macro: {test_f1_macro:.4f}\")\n",
    "print(f\"  F1 Micro: {test_f1_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5989f6bd-f207-47ea-bb97-fdef8551600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Mickey\\AppData\\Local\\Temp\\ipykernel_32808\\901661955.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_small = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='766' max='766' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [766/766 04:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.171570</td>\n",
       "      <td>0.773350</td>\n",
       "      <td>0.855168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.10 → F1=0.6832\n",
      "Threshold=0.15 → F1=0.7693\n",
      "Threshold=0.20 → F1=0.8244\n",
      "Threshold=0.25 → F1=0.8554\n",
      "Threshold=0.30 → F1=0.8732\n",
      "Threshold=0.35 → F1=0.8793\n",
      "Threshold=0.40 → F1=0.8769\n",
      "Threshold=0.45 → F1=0.8692\n",
      "Threshold=0.50 → F1=0.8539\n",
      "Threshold=0.55 → F1=0.8338\n",
      "Threshold=0.60 → F1=0.8061\n",
      "Threshold=0.65 → F1=0.7734\n",
      "Threshold=0.70 → F1=0.7302\n",
      "Threshold=0.75 → F1=0.6781\n",
      "Threshold=0.80 → F1=0.6087\n",
      "Threshold=0.85 → F1=0.5122\n",
      "Threshold=0.90 → F1=0.3777\n",
      "Best threshold (10% model): 0.35\n",
      "Validation F1 (10% model): 0.8793\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% test F1 Macro: 0.8788\n",
      "10% test F1 Micro: 0.9046\n"
     ]
    }
   ],
   "source": [
    "# Data Leakage Sanity Check training on 10% of training set to see if we have leakage or signal\n",
    "train_10pct = ds[\"train\"].train_test_split(test_size=0.9, seed=42)[\"train\"]\n",
    "\n",
    "model_small = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "\n",
    "\n",
    "small_args = TrainingArguments(\n",
    "    output_dir=\"mmf_deberta_v3\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    report_to=[], \n",
    ")\n",
    "\n",
    "trainer_small = Trainer(\n",
    "    model=model_small,\n",
    "    args=small_args,\n",
    "    train_dataset=train_10pct,\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=Compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer_small.train()\n",
    "\n",
    "# Tune on validation set\n",
    "val_out_small = trainer_small.predict(ds[\"validation\"])\n",
    "val_logits_small, val_labels_small = val_out_small.predictions, val_out_small.label_ids\n",
    "val_probs_small = 1 / (1 + np.exp(-val_logits_small))\n",
    "\n",
    "best_thresh_small, best_f1_small, log_small = tune_threshold(val_probs_small, val_labels_small)\n",
    "print(f\"Best threshold (10% model): {best_thresh_small:.2f} (F1={best_f1_small:.3f})\"\n",
    "\n",
    "\n",
    "# Final Eval on test set\n",
    "test_out_small = trainer_small.predict(ds[\"test\"])\n",
    "test_logits_small, test_labels_small = test_out_small.predictions, test_out_small.label_ids\n",
    "test_probs_small = 1 / (1 + np.exp(-test_logits_small))\n",
    "test_preds_small = (test_probs_small >= best_thresh_small).astype(int)\n",
    "\n",
    "test_f1_macro_small = f1_score(test_labels_small, test_preds_small, average=\"macro\", zero_division=0)\n",
    "test_f1_micro_small = f1_score(test_labels_small, test_preds_small, average=\"micro\", zero_division=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0765e1b-abfb-4dda-820e-6550b48a710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to best\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"best\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model/tokenizer\n",
    "trainer.save_model(save_dir)\n",
    "tok.save_pretrained(save_dir)\n",
    "\n",
    "with open(os.path.join(save_dir, \"frames.json\"), \"w\") as f:\n",
    "    json.dump(frames, f, indent=2)\n",
    "with open(os.path.join(save_dir, \"threshold.json\"), \"w\") as f:\n",
    "    json.dump({\"global\": float(best_thresh)}, f, indent=2)\n",
    "with open(os.path.join(save_dir, \"threshold_log.json\"), \"w\") as f:\n",
    "    json.dump(log, f, indent=2)\n",
    "with open(os.path.join(save_dir, \"threshold_log_small.json\"), \"w\") as f:\n",
    "    json.dump(log_small, f, indent=2)\n",
    "\n",
    "print(\"Saved to\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7ddfe-dee1-4815-9e3f-3cbab7628d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
